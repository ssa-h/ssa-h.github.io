<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><link href="/favicon.ico" rel="icon"><meta content="width=device-width,initial-scale=1" name="viewport"><meta content="#000000" name="theme-color"><meta content="Superficial Safety Alignment Hypothesis" name="description"><meta content="index,follow" name="robots"><meta content="website" property="og:type"><meta content="SSAH | Superficial Safety Alignment Hypothesis" property="og:title"><meta content="Project page for the Superficial Safety Alignment Hypothesis (ICLR 2026), including abstract, motivation, and analysis." property="og:description"><link href="/logo192.png" rel="apple-touch-icon"><link href="/manifest.json" rel="manifest"><title>SSAH | Superficial Safety Alignment Hypothesis</title><script defer src="/static/js/main.01f8660f.js"></script><style type="text/css">*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-scroll-snap-strictness:proximity;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:#3b82f680;--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000}::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-scroll-snap-strictness:proximity;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:#3b82f680;--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000}*,:after,:before{border:0 solid #e5e7eb;box-sizing:border-box}:after,:before{--tw-content:""}:host,html{-webkit-text-size-adjust:100%;font-feature-settings:normal;-webkit-tap-highlight-color:transparent;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-variation-settings:normal;line-height:1.5;tab-size:4}body{line-height:inherit}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-feature-settings:normal;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em;font-variation-settings:normal}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:initial}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{font-feature-settings:inherit;color:inherit;font-family:inherit;font-size:100%;font-variation-settings:inherit;font-weight:inherit;letter-spacing:inherit;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:initial;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:initial}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{height:auto;max-width:100%}[hidden]:where(:not([hidden=until-found])){display:none}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}@media (min-width:1536px){.container{max-width:1536px}}.absolute{position:absolute}.relative{position:relative}.sticky{position:-webkit-sticky;position:sticky}.bottom-0{bottom:0}.left-0{left:0}.top-0{top:0}.isolate{isolation:isolate}.z-10{z-index:10}.m-auto{margin:auto}.mx-auto{margin-left:auto;margin-right:auto}.my-6{margin-bottom:1.5rem;margin-top:1.5rem}.mb-4{margin-bottom:1rem}.mb-6{margin-bottom:1.5rem}.mt-2{margin-top:.5rem}.mt-4{margin-top:1rem}.mt-5{margin-top:1.25rem}.mt-6{margin-top:1.5rem}.mt-8{margin-top:2rem}.flex{display:flex}.table{display:table}.hidden{display:none}.h-6{height:1.5rem}.h-64{height:16rem}.h-\[4px\]{height:4px}.h-auto{height:auto}.w-1\/2{width:50%}.w-1\/4{width:25%}.w-6{width:1.5rem}.w-full{width:100%}.max-w-full{max-width:100%}.basis-1\/4{flex-basis:25%}.basis-3\/4{flex-basis:75%}.cursor-pointer{cursor:pointer}.list-inside{list-style-position:inside}.list-decimal{list-style-type:decimal}.list-disc{list-style-type:disc}.flex-col{flex-direction:column}.items-center{align-items:center}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.gap-6{gap:1.5rem}.space-x-4>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(1rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(1rem*var(--tw-space-x-reverse))}.space-x-6>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(1.5rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(1.5rem*var(--tw-space-x-reverse))}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.5rem*var(--tw-space-y-reverse));margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)))}.space-y-4>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(1rem*var(--tw-space-y-reverse));margin-top:calc(1rem*(1 - var(--tw-space-y-reverse)))}.rounded-lg{border-radius:.5rem}.rounded-md{border-radius:.375rem}.border{border-width:1px}.border-l-4{border-left-width:4px}.border-gray-300{--tw-border-opacity:1;border-color:#d1d5db;border-color:rgb(209 213 219/var(--tw-border-opacity,1))}.border-gray-800{--tw-border-opacity:1;border-color:#1f2937;border-color:rgb(31 41 55/var(--tw-border-opacity,1))}.bg-black{--tw-bg-opacity:1;background-color:#000;background-color:rgb(0 0 0/var(--tw-bg-opacity,1))}.bg-gray-50{--tw-bg-opacity:1;background-color:#f9fafb;background-color:rgb(249 250 251/var(--tw-bg-opacity,1))}.bg-gray-800{--tw-bg-opacity:1;background-color:#1f2937;background-color:rgb(31 41 55/var(--tw-bg-opacity,1))}.bg-white{--tw-bg-opacity:1;background-color:#fff;background-color:rgb(255 255 255/var(--tw-bg-opacity,1))}.p-10{padding:2.5rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.py-12{padding-bottom:3rem;padding-top:3rem}.py-2{padding-bottom:.5rem;padding-top:.5rem}.py-4{padding-bottom:1rem;padding-top:1rem}.py-8{padding-bottom:2rem;padding-top:2rem}.pb-2{padding-bottom:.5rem}.pl-4{padding-left:1rem}.text-center{text-align:center}.text-2xl{font-size:1.5rem;line-height:2rem}.text-4xl{font-size:2.25rem;line-height:2.5rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xl{font-size:1.25rem;line-height:1.75rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-medium{font-weight:500}.font-normal{font-weight:400}.font-semibold{font-weight:600}.italic{font-style:italic}.leading-relaxed{line-height:1.625}.leading-tight{line-height:1.25}.text-blue-500{--tw-text-opacity:1;color:#3b82f6;color:rgb(59 130 246/var(--tw-text-opacity,1))}.text-gray-500{--tw-text-opacity:1;color:#6b7280;color:rgb(107 114 128/var(--tw-text-opacity,1))}.text-gray-600{--tw-text-opacity:1;color:#4b5563;color:rgb(75 85 99/var(--tw-text-opacity,1))}.text-gray-700{--tw-text-opacity:1;color:#374151;color:rgb(55 65 81/var(--tw-text-opacity,1))}.text-gray-800{--tw-text-opacity:1;color:#1f2937;color:rgb(31 41 55/var(--tw-text-opacity,1))}.text-gray-900{--tw-text-opacity:1;color:#111827;color:rgb(17 24 39/var(--tw-text-opacity,1))}.text-red-400{--tw-text-opacity:1;color:#f87171;color:rgb(248 113 113/var(--tw-text-opacity,1))}.text-white{--tw-text-opacity:1;color:#fff;color:rgb(255 255 255/var(--tw-text-opacity,1))}.underline{-webkit-text-decoration-line:underline;text-decoration-line:underline}.underline-offset-4{text-underline-offset:4px}.shadow{--tw-shadow:0 1px 3px 0 #0000001a,0 1px 2px -1px #0000001a;--tw-shadow-colored:0 1px 3px 0 var(--tw-shadow-color),0 1px 2px -1px var(--tw-shadow-color)}.shadow,.shadow-md{box-shadow:0 0 #0000,0 0 #0000,var(--tw-shadow);box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-md{--tw-shadow:0 4px 6px -1px #0000001a,0 2px 4px -2px #0000001a;--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color)}.transition{transition-duration:.15s;transition-property:color,background-color,border-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-text-decoration-color,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-text-decoration-color,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1)}.duration-200{transition-duration:.2s}body{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;margin:0}code{font-family:source-code-pro,Menlo,Monaco,Consolas,Courier New,monospace}.hover\:bg-gray-100:hover{--tw-bg-opacity:1;background-color:#f3f4f6;background-color:rgb(243 244 246/var(--tw-bg-opacity,1))}.hover\:bg-gray-700:hover{--tw-bg-opacity:1;background-color:#374151;background-color:rgb(55 65 81/var(--tw-bg-opacity,1))}.hover\:text-gray-900:hover{--tw-text-opacity:1;color:#111827;color:rgb(17 24 39/var(--tw-text-opacity,1))}.focus\:outline-none:focus{outline:2px solid #0000;outline-offset:2px}@media (min-width:768px){.md\:flex{display:flex}.md\:hidden{display:none}.md\:flex-row{flex-direction:row}}</style></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"><div class="App"><header class="bg-gray-50 sticky top-0 z-10 shadow-md"><div class="container mx-auto flex items-center justify-between py-4 px-6"><h1 class="text-2xl font-bold text-gray-700 cursor-pointer">Superficial SA Hypothesis</h1><nav class="hidden md:flex space-x-6 relative"><button class="relative pb-2 text-gray-700 hover:text-gray-900 focus:outline-none transition duration-200">Abstract</button><button class="relative pb-2 text-gray-700 hover:text-gray-900 focus:outline-none transition duration-200">Motivation</button><button class="relative pb-2 text-gray-700 hover:text-gray-900 focus:outline-none transition duration-200">SSAH</button><button class="relative pb-2 text-gray-700 hover:text-gray-900 focus:outline-none transition duration-200">SAH</button><button class="relative pb-2 text-gray-700 hover:text-gray-900 focus:outline-none transition duration-200">Less Is More</button><button class="relative pb-2 text-gray-700 hover:text-gray-900 focus:outline-none transition duration-200">Discussion</button></nav><button class="md:hidden text-gray-700 focus:outline-none"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 6h16M4 12h16m-7 6h7" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"></path></svg></button></div></header><main><div id="abstract"><section class="text-center py-12 bg-gray-50"><h1 class="text-4xl font-bold text-gray-800 leading-tight">Superficial Safety Alignment Hypothesis</h1><section class="text-center py-8 bg-gray-50"><p class="text-lg font-semibold text-gray-800">Jianwei Li & Jung-Eun Kim <span class="text-gray-500">*</span></p><p class="text-gray-700">Department of Computer Science<br>North Carolina State University<br>Raleigh, NC, USA</p><p class="text-gray-600 mt-2">{jli265, jung-eun.kim}@ncsu.edu</p></section><div class="flex justify-center space-x-4"><a class="px-6 py-2 text-sm font-medium text-white bg-gray-800 rounded-md shadow hover:bg-gray-700" href="https://arxiv.org/abs/2410.10862">Paper</a><a class="px-6 py-2 text-sm font-medium text-gray-800 border border-gray-800 rounded-md shadow hover:bg-gray-100" href="https://github.com/JEKimLab/SSAH">Code</a></div><div class="mt-5 text-gray-600 text-sm">Accepted by ICLR 2026</div></section><section class="py-8 bg-gray-50" id="abstract"><div class="container mx-auto px-4"><h2 class="text-2xl font-bold text-gray-800 mb-4 text-center">Overview</h2><p class="text-gray-700 leading-relaxed">As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms.</p><p class="text-gray-700 leading-relaxed mt-4">To bridge the gap, we propose the <strong>Superficial Safety Alignment Hypothesis (SSAH)</strong>, which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components.</p><ul class="list-disc list-inside text-gray-700 mt-4"><li>We conduct an ablation study and identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU).</li><li>Freezing certain safety-critical components <strong>(7.5%)</strong> during fine-tuning allows the model to retain its safety attributes while adapting to new tasks.</li><li>Leveraging redundant units <strong>(20%)</strong> in the pre-trained model as an "alignment budget" can minimize the alignment tax while achieving alignment goals.</li></ul><p class="text-gray-700 leading-relaxed mt-4">All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated at the surface level. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.</p></div></section></div><div id="motivation"><section class="py-8 bg-white" id="motivation"><div class="container mx-auto px-4"><h2 class="text-2xl font-bold text-gray-800 mb-6 text-center">Motivation</h2><div class="flex flex-col md:flex-row items-center gap-6"><div class="basis-3/4"><p class="text-gray-700 leading-relaxed">This study follows a structured three-step approach to address the following critical questions of safety alignment in LLMs. First, we propose a hypothesis aimed at advancing the theoretical understanding of safety alignment. Second, based on this framework, we investigate two fundamental challenges. Finally, leveraging the insights gained, we propose targeted mitigation strategies to address the identified issues.</p><h3 class="text-lg font-semibold text-gray-800 mt-6">Key Questions</h3><div class="mt-4"><p class="text-gray-800 font-bold">Question 1: <span class="font-normal italic">How does safety alignment impact model behavior?</span></p><p class="text-gray-700 leading-relaxed">Through <strong>SSAH</strong>, we posit that safety alignment fundamentally alters a model's decision-making process by teaching an otherwise unsafe model - fulfilling malicious or harmful requests - to follow the correct reasoning pathways. This process can be viewed as a specialized binary classification task - the model must either fulfill the user's request or refuse it based on safety considerations.</p></div><div class="mt-4"><p class="text-gray-800 font-bold">Question 2: <span class="font-normal italic">Why is safety alignment brittle, and why does it introduce an alignment tax?</span></p><p class="text-gray-700 leading-relaxed">We propose an attribute-based approach to analyzing the alignment and fine-tuning processes, where specific attributes are assigned to each individual computational unit - primarily input channels and output neurons. <strong>Our findings explain that the desired attributes can be achieved by repurposing units that were originally responsible for other functions.</strong> This reallocation helps explain both the brittleness of safety mechanisms and alignment tax.</p></div><div class="mt-4"><p class="text-gray-800 font-bold">Question 3: <span class="font-normal italic">Can these issues of safety alignment be mitigated?</span></p><p class="text-gray-700 leading-relaxed">By freezing the safety-critical components during fine-tuning and repurposing redundant units, we can effectively mitigate the brittleness and minimize the alignment tax. <strong>We conclude that the atomic functional unit for safety in LLMs resides at the neuron level and underscores that safety alignment should not be complicated.</strong></p></div></div><div class="basis-1/4 flex justify-center"><img alt="Figure 1" class="max-w-full h-auto rounded-lg shadow-md" src="/static/media/figure1.c5004d2b90b0241b93f0.png"></div></div></div></section></div><div id="ssah"><section class="py-8 bg-gray-50" id="ssah"><div class="container mx-auto px-4"><h2 class="text-2xl font-bold text-gray-800 mb-4 text-center">Superficial Safety Alignment Hypothesis (SSAH)</h2><p class="text-gray-700 leading-relaxed">Previous research introduced the <strong>Superficial Alignment Hypothesis (SAH)</strong>, which posits that a model’s <em>knowledge and capabilities</em> are primarily learned during <em>pretraining</em>, while alignment teaches the model <em>which subdistribution of formats</em> to use when interacting with users. However, this hypothesis focuses on general alignment, making it challenging to isolate the effects of pretraining from those of alignment in cases where a model fails to meet user expectations.</p><p class="text-gray-700 leading-relaxed mt-4">To specifically address safety alignment, we propose the <strong>Superficial Safety Alignment Hypothesis (SSAH)</strong>, which focuses on models capable of fulfilling malicious requests. The hypothesis states:</p><blockquote class="border-l-4 border-gray-300 pl-4 italic text-gray-800 mt-4"><strong>SSAH:</strong> <em>Given an unsafe model that is capable of fulfilling users' malicious requests, <strong>safety alignment</strong> teaches the model the correct <strong>reasoning direction</strong> and a simple refusal mechanism with reserved options.</em></blockquote><p class="text-gray-700 leading-relaxed mt-4"><strong>Reasoning direction</strong> refers to the model's internal decision-making process when responding to malicious queries. It represents the binary choice between fulfilling a harmful request or issuing a refusal, based on safety considerations.</p><h3 class="text-lg font-semibold text-gray-800 mt-6">Key Differences from SAH</h3><ol class="list-decimal list-inside text-gray-700 mt-4 space-y-2"><li><strong>Knowledge and reasoning ability:</strong> SSAH assumes models already possess sufficient knowledge and reasoning abilities, allowing safety alignment to focus solely on ensuring safe behavior without addressing broader capability limitations.</li><li><strong>Refusal mechanisms:</strong> Safety alignment emphasizes standardized refusal formats with fallback options, such as "I cannot fulfill your request as it violates safety guidelines," making the task more straightforward compared to handling diverse human preferences in general alignment.</li><li><strong>Correction of reasoning direction:</strong> SSAH aims to teach the model to consistently choose the correct reasoning direction, which can be framed as a binary classification task—either fulfilling or refusing a user request based on its safety implications.</li></ol><h3 class="text-lg font-semibold text-gray-800 mt-8">Challenges in Proving SSAH</h3><p class="text-gray-700 leading-relaxed mt-4">Empirically proving SSAH remains challenging due to the infeasibility of sampling sufficient outputs to fully capture the model’s distribution of responses. Specifically, it is difficult to draw comprehensive conclusions solely from surface-level benchmark evaluations.</p><p class="text-gray-700 leading-relaxed mt-4">To address this limitation, we take an <strong>alternative approach</strong>: <strong class="text-red-400">if SSAH holds,</strong> we should observe distinct and consistent differences in the reasoning direction at each step of generation between safety-aligned and non-safety-aligned models. In a safety-aligned model, the reasoning direction should consistently guide the model in rejecting harmful queries at every token generation step. In contrast, a non-safety-aligned model might exhibit reasoning patterns that lean toward fulfilling malicious requests. Rather than relying solely on surface-level benchmark evaluations, we can probe the model’s reasoning direction to gain deeper insights into its internal decision-making process at each step regardless of the specific outputs produced.</p><h3 class="text-lg font-semibold text-gray-800 mt-8">Probing Experiment</h3><p class="text-gray-700 leading-relaxed mt-4">We designed a probing experiment to infer the model’s reasoning direction by comparing the <strong>hidden state distances</strong> in feature space across three types of queries:</p><ol class="list-decimal list-inside text-gray-700 mt-4 space-y-2"><li><strong>Query:</strong> A malicious query (e.g., "How to make a bomb?").</li><li><strong>Query + benign prompt tokens:</strong> The malicious query followed by benign tokens (e.g., "Sorry, I can’t...").</li><li><strong>Query + malicious prompt tokens:</strong> The malicious query followed by malicious tokens (e.g., "Here’s how...").</li></ol><p class="text-gray-700 leading-relaxed mt-4">By comparing the distances between hidden states, we gain insights into how safety alignment reshapes the model’s decision-making process during token generation. Specifically, we expect aligned models to show shorter distances between the <strong>Query</strong> and <strong>Query + benign prompt tokens</strong>, reflecting a preference for safe reasoning.</p><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="Figure 2" class="w-1/2 m-auto h-auto" src="/static/media/figure2.d9d958eebb6a846af5a9.png"></div><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="Figure 7" class="w-1/2 m-auto h-auto" src="/static/media/figure7.944a22be4a9df56184b6.png"></div><h3 class="text-lg font-semibold text-gray-800 mt-8">Results Analysis</h3><p class="text-gray-700 leading-relaxed mt-4">The experiment results confirm that safety alignment influences the model’s reasoning direction at each step of generation:</p><ul class="list-disc list-inside text-gray-700 mt-4 space-y-2"><li>In aligned models, the distance between <strong>Query</strong> and <strong>Query + benign prompt tokens</strong> is consistently shorter than the distance to <strong>Query + malicious prompt tokens</strong>.</li><li>In unaligned models, the opposite pattern is observed, indicating a lack of strong preference for safe reasoning.</li><li>Aligned models exhibit clear and consistent safe reasoning preferences across all transformer blocks, whereas unaligned models demonstrate less pronounced differences.</li></ul><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="Figure 3" class="w-1/4 m-auto h-auto" src="/static/media/figure3.f59ca771d1fc60ba452d.png"></div><h3 class="text-lg font-semibold text-gray-800 mt-8">Discussion and Implications</h3><p class="text-gray-700 leading-relaxed mt-4">The results highlight that safety alignment not only influences higher-level features in later layers but also embeds safe reasoning preferences in earlier layers of the transformer architecture. This suggests that safety alignment operates at multiple levels, fundamentally reshaping the model’s internal decision-making process to ensure safer behavior throughout the response generation process.</p><p class="text-gray-700 leading-relaxed mt-4"><strong>While these findings provide strong evidence supporting SSAH, it is important to note that they do not fully capture the nuanced changes introduced by safety alignment. Further research is needed to explore other potential effects and limitations.</strong></p><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="Figure 4" class="w-1/4 m-auto h-auto" src="/static/media/figure4.f92284864f75fd4e0559.png"></div></div></section></div><div id="sah"><section class="py-8 bg-white" id="sah"><div class="container mx-auto px-4"><h2 class="text-2xl font-bold text-gray-800 mb-4 text-center">Safety Alignment Hypothesis (SAH) for Jailbreak/Red-teaming Attaks</h2><p class="text-gray-700 leading-relaxed">The <strong>Superficial Safety Alignment Hypothesis (SSAH)</strong> was originally proposed to explain how safety alignment impacts model behavior under direct attacks. However, our research demonstrates that SSAH can extend beyond direct attacks to provide theoretical guidance for addressing jailbreak and red-teaming scenarios.</p><h3 class="text-lg font-semibold text-gray-800 mt-6">Evolving from SSAH to SAH</h3><p class="text-gray-700 leading-relaxed mt-4">Building on the foundations of SSAH, we propose the <strong>Safety Alignment Hypothesis (SAH)</strong>, which refines and extends SSAH to encompass all stages of model generation. SAH represents a comprehensive framework for understanding how alignment techniques should influence model behavior to ensure safety across all interaction steps.</p><blockquote class="border-l-4 border-gray-300 pl-4 italic text-gray-800 mt-4"><strong>Safety Alignment Hypothesis (SAH):</strong><em>Given an unsafe model capable of fulfilling users' malicious requests, safety alignment should teach the model to choose and maintain the correct reasoning direction at each generation step, along with simple refusal mechanisms. This allows the model to continuously re-evaluate and re-choose the reasoning direction throughout the interaction.</em></blockquote><h3 class="text-lg font-semibold text-gray-800 mt-6">Theoretical Contributions</h3><ul class="list-disc list-inside text-gray-700 mt-4 space-y-2"><li>SAH provides a theoretical framework for improving safety alignment by equipping models with mechanisms to maintain the correct reasoning direction across all generated tokens.</li><li>This hypothesis offers a conceptual pathway to mitigate jailbreak attacks by ensuring safety mechanisms persist even under adversarial attempts.</li><li>SAH bridges the gap between existing alignment techniques and their limitations, offering a roadmap for future advancements in robust and scalable safety alignment.</li></ul></div></section></div><div id="lessismore"><section class="py-8 bg-gray-50" id="less-is-more"><div class="container mx-auto px-4"><h2 class="text-2xl font-bold text-gray-800 mb-6 text-center">Less is More for Safety Alignment</h2><p class="text-gray-700 leading-relaxed">Based on the <strong>Superficial Safety Alignment Hypothesis (SSAH)</strong>, we posit that safety alignment only needs to teach the model the correct reasoning direction - either fulfilling or refusing a request - and to equip it with a standard refusal mechanism. This leads to the insight that <strong>safety alignment can be achieved using only a small subset of critical computing units, as the task can be interpreted as a binary classification combined with a multi-selection task.</strong></p><h3 class="text-xl font-semibold text-gray-800 mt-8">Identifying Safety-Critical Computing Units</h3><p class="text-gray-700 leading-relaxed mt-4">To validate this hypothesis, we categorized the computing units of large language models (LLMs) into four groups:</p><ul class="list-disc list-inside text-gray-700 mt-4 space-y-2"><li><strong>Exclusive Safety Units (ESU):</strong> Linked exclusively (relatively) to the safety attribute.</li><li><strong>Exclusive Utility Units (EUU):</strong> Linked exclusively (relatively) to the utility attribute.</li><li><strong>Complex Units (CU):</strong> Contribute to both safety and utility attributes.</li><li><strong>Redundant Units (RU):</strong> Not associated with any attribute.</li></ul><p class="text-gray-700 leading-relaxed mt-4">To verify that different groups of computing units contribute exclusively, collectively, or neither to safety and utility attributes, we use a <strong>model pruning mechanism</strong>. The rationale behind pruning is that removing components most closely linked to a specific attribute would significantly impact the model’s performance in that area - it is a sort of ablation study. As pruning reduces the model’s capacity, the most affected attributes reveal the critical components for that function.</p><div class="my-6 w-full h-auto bg-white p-10 flex items-center justify-center"><img alt="Table 1" class="w-1/2 m-auto h-auto" src="/static/media/table1.d26eaa1cfcf0272a18d4.png"></div><p class="text-gray-700 leading-relaxed mt-4">Our experiments reveal that only <strong>1.3–1.4%</strong> of the model's units are exclusively responsible for safety attributes, confirming that safety alignment relies on a minimal subset of safety-critical components. Complex units play a supportive role by contributing to both safety and utility tasks, while redundant units have no significant impact.</p><h3 class="text-xl font-semibold text-gray-800 mt-8">Why is Safety Brittle?</h3><p class="text-gray-700 leading-relaxed mt-4">Fine-tuning safety-aligned models for new tasks often compromises their safety performance. During fine-tuning, safety-critical units and complex units tend to be repurposed for utility tasks, weakening the model’s safety guardrails. This phenomenon highlights the inherent brittleness of current safety alignment methods.</p><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="Figure 5" class="w-1/2 m-auto h-auto" src="/static/media/figure5.ba0e3d1e47bba787d111.png"></div><h3 class="text-lg font-semibold text-gray-800 mt-8">Freezing Safety-Critical Components</h3><p class="text-gray-700 leading-relaxed mt-4">To address this brittleness, we propose freezing safety-critical components, including ESUs and top-performing complex units, during fine-tuning. Experimental results demonstrate that this approach significantly preserves safety performance while minimizing degradation in safety guardrails.</p><div class="my-6 w-full h-auto bg-white p-10 flex items-center justify-center"><img alt="Table 2" class="w-1/2 m-auto h-auto" src="/static/media/table2.11485933ee61af1de390.png"></div><h3 class="text-lg font-semibold text-gray-800 mt-8">Comparison with Parameter-Efficient Fine-Tuning (PEFT)</h3><p class="text-gray-700 leading-relaxed mt-4">Our approach outperforms parameter-efficient fine-tuning methods such as <code>LoRA</code>, <code>LLaMA-Adapter</code>, and <code>Prefix Tuning</code>, which degrade safety performance more severely. This confirms that the preservation of safety is not merely due to freezing parameters but results from accurately identifying and protecting safety-critical components.</p><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="table 3" class="w-1/2 m-auto h-auto" src="/static/media/table3.4f7e3014a41fd2010fff.png"></div><h3 class="text-lg font-semibold text-gray-800 mt-8">Free Lunch: Repurposing Redundant Units as Alignment Budget</h3><p class="text-gray-700 leading-relaxed mt-4">We extend our insights to explore whether redundant units (RUs)—which account for at least <strong>20%</strong> of parameters in pre-trained LLMs—can be repurposed as a budget for safety alignment. Our hypothesis is that fine-tuning these redundant units can enhance safety alignment while reducing alignment tax.</p><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="Figure 6" class="w-1/2 m-auto h-auto" src="/static/media/figure6.7c9da8d0e1c49707c9b5.png"></div><h3 class="text-lg font-semibold text-gray-800 mt-8">Experimental Results</h3><p class="text-gray-700 leading-relaxed mt-4">Using the pruning method, we identified redundant units in LLaMA-7B and fine-tuned only these units for alignment. The results demonstrate that alignment can be achieved with updates to just <strong>20% of the model's parameters</strong>, effectively eliminating alignment tax. This finding highlights the scalability and efficiency of our approach, making it a promising direction for future LLM safety alignment.</p><div class="my-6 w-full h-auto p-10 bg-white flex items-center justify-center"><img alt="table 4" class="w-1/2 m-auto h-auto" src="/static/media/table4.b46a44dddacb2b15e2fb.png"></div></div></section></div><div id="discussion"><section class="py-8 bg-white" id="discussion"><div class="container mx-auto px-4"><h2 class="text-2xl font-bold text-gray-800 mb-6 text-center">Discussion, Limitation, and Conclusion</h2><h3 class="text-lg font-semibold text-gray-800 mt-6">Discussion</h3><p class="text-gray-700 leading-relaxed mt-4">While our <strong>Safety Alignment Hypothesis (SAH)</strong> provides valuable insights into adversarial scenarios, such as jailbreak attacks, this work does not propose a specific solution to address these issues. If these challenges could be resolved within the framework of our theory, the term "Superficial" in SSAH may no longer be necessary.</p><p class="text-gray-700 leading-relaxed mt-4">Recent research <a class="text-blue-500 underline" href="#">[Qi et al., 2024]</a> offers supporting evidence for this perspective. However, advanced adversarial attacks may not be fully mitigated by relying solely on the model’s internal mechanisms. A systematic, multi-layered approach extending beyond the model itself may be required to effectively defend against sophisticated threats.</p><h3 class="text-lg font-semibold text-gray-800 mt-6">Limitation</h3><p class="text-gray-700 leading-relaxed mt-4">In reallocating redundant units for safety purposes, we only evaluated the impact of the alignment method <strong>Supervised Fine-Tuning (SFT)</strong>. Due to resource constraints, this study did not explore other alignment methods, such as <strong>Proximal Policy Optimization (PPO)</strong> or <strong>Direct Preference Optimization (DPO)</strong>. Future work could expand on these methods to validate the generalizability of our approach.</p><h3 class="text-lg font-semibold text-gray-800 mt-6">Conclusion</h3><p class="text-gray-700 leading-relaxed mt-4">This paper distinguishes safety alignment from general alignment in large language models (LLMs) and addresses three critical questions:</p><ul class="list-disc list-inside text-gray-700 mt-4 space-y-2"><li>How does safety alignment affect model behavior?</li><li>Why are safety mechanisms brittle?</li><li>How can the safety alignment tax be mitigated?</li></ul><p class="text-gray-700 leading-relaxed mt-4">By systematically answering these questions, we demonstrate that safety alignment can be a straightforward and efficient process, providing a foundation for more robust and scalable safety mechanisms in future LLMs.</p></div></section></div></main><footer class="bg-gray-50 h-64 flex items-center justify-center" id="footer"><div class="text-center"><p class="text-gray-600 text-sm">© 2024 Kim Lab. All rights reserved.</p><p class="text-gray-500 text-xs mt-2">Designed with ❤️ using React and Tailwind CSS</p></div></footer></div></div></body></html>